{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'AIzaSyDKKx2-1z951_dWEWYm8-Tf8gK0oztRIDc'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "analyze_request = {\n",
        "  'comment': { 'text': 'welcome to the program' },\n",
        "  'requestedAttributes': {'TOXICITY': {}}\n",
        "}\n",
        "\n",
        "response = client.comments().analyze(body=analyze_request).execute()\n",
        "print(response)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGA7nm259xfs",
        "outputId": "c6ce3bad-29c8-4879-fbe0-f78824d5c3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 22, 'score': {'value': 0.023199737, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.023199737, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attribute_dict = (response['attributeScores'])\n",
        "#print(attribute_dict)\n",
        "\n",
        "toxicity_dict = (attribute_dict['TOXICITY'])\n",
        "# print(toxicity_dict)\n",
        "\n",
        "summary_dict = (toxicity_dict['summaryScore'])\n",
        "# print(span_dict)\n",
        "\n",
        "value_dict = summary_dict ['value']\n",
        "print(value_dict)\n",
        "\n",
        "\n",
        "#This cell allows me to identify the value of the summaryScore rather than manually sorting through the dictionary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWLBV2Gl_9MW",
        "outputId": "c633161f-d38e-484f-9b3c-ef0b1ae53ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.023199737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\"wow that looks badass\", \"wow that shit is so good\", \"that's sick\", \"Those tricks are fucking awesome\", \"Dude that's wicked\", \"crap\", \"that's not cool\", \"that's so stupid\", \"I can't believe this\", \"that is disgusting\", \"I hate john\", \"This party is the shit\", \"this is pathetic\", \"fucking cool\"]\n",
        "\n",
        "for val in range (0, len(list_of_prompts)):\n",
        "\n",
        "  analyze_request = {\n",
        "  'comment': { 'text': list_of_prompts[val] },\n",
        "  'requestedAttributes': {'TOXICITY': {}}\n",
        "}\n",
        "\n",
        "  response = client.comments().analyze(body=analyze_request).execute()\n",
        "\n",
        "  attribute_dict = (response['attributeScores'])\n",
        "\n",
        "  toxicity_dict = (attribute_dict['TOXICITY'])\n",
        "\n",
        "  summary_dict = (toxicity_dict['summaryScore'])\n",
        "\n",
        "\n",
        "  value_dict = summary_dict ['value']\n",
        "\n",
        "\n",
        "  if value_dict > 0.5:\n",
        "    toxic_or_not = \"Toxic\"\n",
        "  else:\n",
        "    toxic_or_not = \"Not Toxic\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(list_of_prompts[val], value_dict, toxic_or_not)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# The prompts that are used for this experiment can be viewed as a list in list_of_prompts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js5XaTd0E-oy",
        "outputId": "ed085b2a-5b8a-445d-de53-fe7c41d50e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wow that looks badass 0.4269174 Not Toxic\n",
            "wow that shit is so good 0.6342749 Toxic\n",
            "that's sick 0.3250515 Not Toxic\n",
            "Those tricks are fucking awesome 0.5721988 Toxic\n",
            "Dude that's wicked 0.31963667 Not Toxic\n",
            "crap 0.5532103 Toxic\n",
            "that's not cool 0.059974585 Not Toxic\n",
            "that's so stupid 0.76523775 Toxic\n",
            "I can't believe this 0.046368107 Not Toxic\n",
            "that is disgusting 0.39915034 Not Toxic\n",
            "I hate john 0.54823303 Toxic\n",
            "This party is the shit 0.911035 Toxic\n",
            "this is pathetic 0.48021027 Not Toxic\n",
            "fucking cool 0.60152835 Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The prompts from the list along with their score can be viewed above. I have also opted to add a statement that would inform the user regarding whether or not the phrase could be categorized as \"Toxic\" or \"Not Toxic\""
      ],
      "metadata": {
        "id": "GFCtbPNOHM87"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}