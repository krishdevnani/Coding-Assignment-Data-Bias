{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'AIzaSyDKKx2-1z951_dWEWYm8-Tf8gK0oztRIDc'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "analyze_request = {\n",
        "  'comment': { 'text': 'welcome to the program' },\n",
        "  'requestedAttributes': {'TOXICITY': {}}\n",
        "}\n",
        "\n",
        "response = client.comments().analyze(body=analyze_request).execute()\n",
        "print(response)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGA7nm259xfs",
        "outputId": "2d33d8ea-d807-4f17-eaa4-d561a81076a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 22, 'score': {'value': 0.023199737, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.023199737, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attribute_dict = (response['attributeScores'])\n",
        "#print(attribute_dict)\n",
        "\n",
        "toxicity_dict = (attribute_dict['TOXICITY'])\n",
        "# print(toxicity_dict)\n",
        "\n",
        "summary_dict = (toxicity_dict['summaryScore'])\n",
        "# print(span_dict)\n",
        "\n",
        "value_dict = summary_dict ['value']\n",
        "print(value_dict)\n",
        "\n",
        "\n",
        "#This cell allows me to identify the value of the summaryScore rather than manually sorting through the dictionary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWLBV2Gl_9MW",
        "outputId": "b29dfb58-7df9-4a44-c614-ef442cec6019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.023199737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\"wow that looks badass\", \"wow that shit is so good\", \"that's sick\", \"Those tricks are fucking awesome\", \"Dude that's wicked\", \"crap\", \"that's so horrible\", \"that's so stupid\", \"I can't believe this\", \"that is disgusting\", \"I hate john\", \"This party is the shit\", \"this is pathetic\", \"fucking cool\"]\n",
        "toxic_actual = [\"Not Toxic\", \"Not Toxic\", \"Not Toxic\", \"Not Toxic\", \"Not Toxic\", \"Toxic\", \"Toxic\", \"Toxic\", \"Toxic\", \"Toxic\", \"Toxic\", \"Not Toxic\", \"Toxic\", \"Not Toxic\"]\n",
        "accurate_counter = 0\n",
        "\n",
        "for val in range (0, len(list_of_prompts)):\n",
        "\n",
        "  analyze_request = {\n",
        "  'comment': { 'text': list_of_prompts[val] },\n",
        "  'requestedAttributes': {'TOXICITY': {}}\n",
        "}\n",
        "\n",
        "  response = client.comments().analyze(body=analyze_request).execute()\n",
        "\n",
        "  attribute_dict = (response['attributeScores'])\n",
        "\n",
        "  toxicity_dict = (attribute_dict['TOXICITY'])\n",
        "\n",
        "  summary_dict = (toxicity_dict['summaryScore'])\n",
        "\n",
        "\n",
        "  value_dict = summary_dict ['value']\n",
        "\n",
        "\n",
        "  if value_dict > 0.5:\n",
        "    toxic_or_not = \"Toxic\"\n",
        "  else:\n",
        "    toxic_or_not = \"Not Toxic\"\n",
        "\n",
        "\n",
        "\n",
        "  if toxic_actual[val] == toxic_or_not:\n",
        "    accurate_counter = accurate_counter + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#make a counter for predictions\n",
        "\n",
        "  print(list_of_prompts[val], value_dict, \"The predictor value is\", toxic_or_not, \".The actual value is\", toxic_actual[val])\n",
        "\n",
        "accurate_percentage = (accurate_counter / len(toxic_actual)) * 100\n",
        "print(\"The Percentage accurate for google perspective api predictor is \", accurate_percentage,\"%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# The prompts that are used for this experiment can be viewed as a list in list_of_prompts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js5XaTd0E-oy",
        "outputId": "d6fa5a45-e5cb-4334-da88-e8bde9de1cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wow that looks badass 0.4269174 The predictor value is Not Toxic .The actual value is Not Toxic\n",
            "wow that shit is so good 0.6342749 The predictor value is Toxic .The actual value is Not Toxic\n",
            "that's sick 0.3250515 The predictor value is Not Toxic .The actual value is Not Toxic\n",
            "Those tricks are fucking awesome 0.5721988 The predictor value is Toxic .The actual value is Not Toxic\n",
            "Dude that's wicked 0.31963667 The predictor value is Not Toxic .The actual value is Not Toxic\n",
            "crap 0.5532103 The predictor value is Toxic .The actual value is Toxic\n",
            "that's so horrible 0.3389984 The predictor value is Not Toxic .The actual value is Toxic\n",
            "that's so stupid 0.76523775 The predictor value is Toxic .The actual value is Toxic\n",
            "I can't believe this 0.046368107 The predictor value is Not Toxic .The actual value is Toxic\n",
            "that is disgusting 0.39915034 The predictor value is Not Toxic .The actual value is Toxic\n",
            "I hate john 0.54823303 The predictor value is Toxic .The actual value is Toxic\n",
            "This party is the shit 0.911035 The predictor value is Toxic .The actual value is Not Toxic\n",
            "this is pathetic 0.48021027 The predictor value is Not Toxic .The actual value is Toxic\n",
            "fucking cool 0.60152835 The predictor value is Toxic .The actual value is Not Toxic\n",
            "The Percentage accurate for google perspective api predictor is  42.857142857142854 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The prompts from the list along with their score can be viewed above. I have also opted to add a statement that would inform the user regarding whether or not the phrase could be categorized as \"Toxic\" or \"Not Toxic\""
      ],
      "metadata": {
        "id": "GFCtbPNOHM87"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}